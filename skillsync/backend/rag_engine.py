import os
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from backend.database import load_vectorstore
from flashrank import Ranker, RerankRequest

# Model √©s Reranker inicializ√°l√°sa
llm = ChatOllama(model="llama3.2:3b", temperature=0)
# CPU-bar√°t, gyors reranker
ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="opt/flashrank")

# --- QUERY EXPANSION PROMPT ---
expansion_prompt = ChatPromptTemplate.from_template("""
Te egy AI asszisztens vagy. Gener√°lj 3 k√ºl√∂nb√∂z≈ë v√°ltozatot a k√©rd√©sb≈ël magyarul.
Eredeti k√©rd√©s: {question}
Csak a 3 k√©rd√©st add meg, sz√°moz√°s n√©lk√ºl, soronk√©nt!
""")
expansion_chain = expansion_prompt | llm | StrOutputParser()

def get_streaming_response(question, history, tenant_id="default"):
    # 1. K√©rd√©s b≈ëv√≠t√©se
    try:
        expanded_text = expansion_chain.invoke({"question": question})
        queries = [question] + [q.strip() for q in expanded_text.strip().split("\n") if q.strip()][:3]
    except:
        queries = [question]

    # 2. Dokumentumok begy≈±jt√©se a hibrid keres≈ëvel
    retriever = load_vectorstore(tenant_id)
    if not retriever:
        yield "M√©g nincsenek felt√∂lt√∂tt dokumentumok."
        return

    all_docs = []
    for q in queries:
        all_docs.extend(retriever.invoke(q))
    
    unique_docs = {doc.page_content: doc for doc in all_docs}.values()
    
    # 3. RERANKING
    ranker_input = [
        {"id": i, "text": doc.page_content, "meta": doc.metadata} 
        for i, doc in enumerate(unique_docs)
    ]
    
    rerank_request = RerankRequest(query=question, passages=ranker_input)
    results = ranker.rerank(rerank_request)
    
    top_results = results[:3] 
    
    context_parts = []
    sources = []
    
    for res in top_results:
        context_parts.append(res['text'])
        
        # A FlashRank a metaadatokat √°ltal√°ban k√∂zvetlen√ºl a 'meta' kulcsba teszi,
        # de n√©ha √©rdemes ellen≈ërizni, hogy l√©tezik-e
        meta = res.get('meta', {})
        if not meta: # Biztons√°gi ment≈ë√∂v, ha a meta √ºres lenne
            meta = {k: v for k, v in res.items() if k not in ['id', 'text', 'score']}
            
        fname = meta.get('filename', 'Ismeretlen f√°jl')
        page = meta.get('page', '?')
        
        try:
            # Kezelj√ºk, ha a page None vagy nem sz√°m
            page_num = int(float(page)) + 1 if page != '?' else '?'
        except (ValueError, TypeError):
            page_num = page
            
        sources.append(f"{fname} (oldal: {page_num})")

    context = "\n\n".join(context_parts)
    unique_sources = sorted(list(set(sources)))

    # 4. V√°lasz gener√°l√°sa
    qa_prompt = ChatPromptTemplate.from_template("""
    Haszn√°ld az al√°bbi kontextust. V√°laszolj magyarul.
    Kontextus: {context}
    K√©rd√©s: {question}
    """)
    
    final_chain = qa_prompt | llm | StrOutputParser()
    
    for chunk in final_chain.stream({"context": context, "question": question}):
        yield chunk

    # A stream v√©g√©n k√ºldj√ºk el a forr√°sokat
    if unique_sources:
        yield f"\n\nüìö **Forr√°sok:** {', '.join(unique_sources)}"

def get_response(query, history=None, tenant_id="default"):
    """
    Szinkron v√°ltozat. √ñsszegy≈±jti a stream minden darabj√°t, 
    bele√©rtve a v√©g√©re f≈±z√∂tt forr√°sokat is.
    """
    if history is None: 
        history = []
    
    full_response = ""
    # V√©gigzongor√°zzuk a gener√°tort
    for chunk in get_streaming_response(query, history, tenant_id):
        full_response += chunk
    
    # Itt m√°r nem kell k√ºl√∂n lista a forr√°soknak, 
    # mert a full_response tartalmazza a "üìö Forr√°sok" r√©szt a v√©g√©n.
    return full_response