# backend/database.py
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
import os

# Lok√°lis embedding modell konfigur√°ci√≥ja
# Fontos: El≈ëtte futtasd: ollama pull nomic-embed-text
embedding_model = OllamaEmbeddings(model="nomic-embed-text")

VECTOR_DB_PATH = "vectorstore/db_faiss"

def create_or_update_vectorstore(chunks):
    """Vektoradatb√°zis l√©trehoz√°sa vagy friss√≠t√©se lok√°lisan."""
    
    if os.path.exists(VECTOR_DB_PATH):
        # Ha m√°r l√©tezik, bet√∂ltj√ºk √©s hozz√°adjuk az √∫jat
        vector_db = FAISS.load_local(
            VECTOR_DB_PATH, 
            embedding_model, 
            allow_dangerous_deserialization=True # Lok√°lis k√∂rnyezetben biztons√°gos
        )
        vector_db.add_documents(chunks)
    else:
        # √öj adatb√°zis l√©trehoz√°sa
        vector_db = FAISS.from_documents(chunks, embedding_model)

    # Ment√©s a vectorstore/db_faiss mapp√°ba
    vector_db.save_local(VECTOR_DB_PATH)
    print(f"üíæ Vektoradatb√°zis elmentve ide: {VECTOR_DB_PATH}")
    return vector_db

def load_vectorstore():
    """L√©tez≈ë adatb√°zis bet√∂lt√©se keres√©shez."""
    if os.path.exists(VECTOR_DB_PATH):
        return FAISS.load_local(
            VECTOR_DB_PATH, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
    return None